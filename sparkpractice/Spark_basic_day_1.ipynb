{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e62225e",
   "metadata": {},
   "source": [
    "\n",
    "# üìò Spark DataFrame Reading & Schema Handling - Notes & Tips\n",
    "\n",
    "## üîç Data Reading Modes in Spark\n",
    "When reading data using Spark, the `.option(\"mode\", \"<MODE>\")` setting controls how Spark handles malformed or corrupt rows:\n",
    "\n",
    "- **PERMISSIVE (default)**: Spark includes corrupt records with nulls and stores raw data in a special column `_corrupt_record`.\n",
    "- **DROPMALFORMED**: Drops rows that don't match the schema.\n",
    "- **FAILFAST**: Fails immediately if it encounters bad records.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    ".option(\"mode\", \"PERMISSIVE\")     # Tolerant: fills in nulls, logs bad ones\n",
    ".option(\"mode\", \"DROPMALFORMED\")  # Skips bad records\n",
    ".option(\"mode\", \"FAILFAST\")       # Throws error on first bad record\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Handling Corrupt Records\n",
    "Use the `badRecordsPath` option to store invalid records for debugging.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    ".option(\"badRecordsPath\", \"/FileStore/tables/bad_records\")\n",
    "```\n",
    "\n",
    "Spark saves malformed rows as JSON, which you can later read like:\n",
    "```python\n",
    "spark.read.json(\"/FileStore/tables/bad_records/<timestamp-folder>\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Schema Creation Techniques\n",
    "\n",
    "### 1. **DDL Format (String schema)**\n",
    "```python\n",
    "schema = \"id INT, name STRING, age INT, salary INT, address STRING, nominee STRING\"\n",
    "```\n",
    "\n",
    "### 2. **StructType Programmatic Schema**\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"nominee\", StringType(), True)\n",
    "])\n",
    "```\n",
    "\n",
    "### 3. **Infer Schema (not recommended for production)**\n",
    "```python\n",
    ".option(\"inferSchema\", \"true\")  # Spark guesses the schema\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "These techniques help ensure your DataFrame reads are robust and ready for real-world messy data. Use `FAILFAST` in pipelines that need to abort on error, and `badRecordsPath` when you want to log & analyze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b58e64e-3a70-41c6-83d7-75913ed1dac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3238576104590447#setting/sparkui/0419-190959-k5u8ql1f/driver-3411625316787097611\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3238576104590447#setting/sparkui/0419-190959-k5u8ql1f/driver-3411625316787097611\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7b3bd5-cf2d-4af7-add5-9ea79c3b9edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|                 _c0|                _c1|  _c2|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"false\")\\\n",
    "# Setting read options\n",
    "    .option(\"inferschema\",\"false\")\\\n",
    "# Setting read options\n",
    "        .option(\"mode\",\"FAILFAST\")\\\n",
    "# Loading the data from specified path\n",
    "        .load(\"/FileStore/tables/2015_summary.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b23e0a8-f4c5-438f-8513-1dfd50810da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th></tr></thead><tbody><tr><td>DEST_COUNTRY_NAME</td><td>ORIGIN_COUNTRY_NAME</td><td>count</td></tr><tr><td>United States</td><td>Romania</td><td>15</td></tr><tr><td>United States</td><td>Croatia</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>344</td></tr><tr><td>Egypt</td><td>United States</td><td>15</td></tr><tr><td>United States</td><td>India</td><td>62</td></tr><tr><td>United States</td><td>Singapore</td><td>1</td></tr><tr><td>United States</td><td>Grenada</td><td>62</td></tr><tr><td>Costa Rica</td><td>United States</td><td>588</td></tr><tr><td>Senegal</td><td>United States</td><td>40</td></tr><tr><td>Moldova</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Sint Maarten</td><td>325</td></tr><tr><td>United States</td><td>Marshall Islands</td><td>39</td></tr><tr><td>Guyana</td><td>United States</td><td>64</td></tr><tr><td>Malta</td><td>United States</td><td>1</td></tr><tr><td>Anguilla</td><td>United States</td><td>41</td></tr><tr><td>Bolivia</td><td>United States</td><td>30</td></tr><tr><td>United States</td><td>Paraguay</td><td>6</td></tr><tr><td>Algeria</td><td>United States</td><td>4</td></tr><tr><td>Turks and Caicos Islands</td><td>United States</td><td>230</td></tr><tr><td>United States</td><td>Gibraltar</td><td>1</td></tr><tr><td>Saint Vincent and the Grenadines</td><td>United States</td><td>1</td></tr><tr><td>Italy</td><td>United States</td><td>382</td></tr><tr><td>United States</td><td>Federated States of Micronesia</td><td>69</td></tr><tr><td>United States</td><td>Russia</td><td>161</td></tr><tr><td>Pakistan</td><td>United States</td><td>12</td></tr><tr><td>United States</td><td>Netherlands</td><td>660</td></tr><tr><td>Iceland</td><td>United States</td><td>181</td></tr><tr><td>Marshall Islands</td><td>United States</td><td>42</td></tr><tr><td>Luxembourg</td><td>United States</td><td>155</td></tr><tr><td>Honduras</td><td>United States</td><td>362</td></tr><tr><td>The Bahamas</td><td>United States</td><td>955</td></tr><tr><td>United States</td><td>Senegal</td><td>42</td></tr><tr><td>El Salvador</td><td>United States</td><td>561</td></tr><tr><td>Samoa</td><td>United States</td><td>25</td></tr><tr><td>United States</td><td>Angola</td><td>13</td></tr><tr><td>Switzerland</td><td>United States</td><td>294</td></tr><tr><td>United States</td><td>Anguilla</td><td>38</td></tr><tr><td>Sint Maarten</td><td>United States</td><td>325</td></tr><tr><td>Hong Kong</td><td>United States</td><td>332</td></tr><tr><td>Trinidad and Tobago</td><td>United States</td><td>211</td></tr><tr><td>Latvia</td><td>United States</td><td>19</td></tr><tr><td>United States</td><td>Ecuador</td><td>300</td></tr><tr><td>Suriname</td><td>United States</td><td>1</td></tr><tr><td>Mexico</td><td>United States</td><td>7140</td></tr><tr><td>United States</td><td>Cyprus</td><td>1</td></tr><tr><td>Ecuador</td><td>United States</td><td>268</td></tr><tr><td>United States</td><td>Portugal</td><td>134</td></tr><tr><td>United States</td><td>Costa Rica</td><td>608</td></tr><tr><td>United States</td><td>Guatemala</td><td>318</td></tr><tr><td>United States</td><td>Suriname</td><td>34</td></tr><tr><td>Colombia</td><td>United States</td><td>873</td></tr><tr><td>United States</td><td>Cape Verde</td><td>14</td></tr><tr><td>United States</td><td>Jamaica</td><td>712</td></tr><tr><td>Norway</td><td>United States</td><td>121</td></tr><tr><td>United States</td><td>Malaysia</td><td>3</td></tr><tr><td>United States</td><td>Morocco</td><td>19</td></tr><tr><td>Thailand</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>Samoa</td><td>25</td></tr><tr><td>Venezuela</td><td>United States</td><td>290</td></tr><tr><td>United States</td><td>Palau</td><td>31</td></tr><tr><td>United States</td><td>Venezuela</td><td>246</td></tr><tr><td>Panama</td><td>United States</td><td>510</td></tr><tr><td>Antigua and Barbuda</td><td>United States</td><td>126</td></tr><tr><td>United States</td><td>Chile</td><td>185</td></tr><tr><td>Morocco</td><td>United States</td><td>15</td></tr><tr><td>United States</td><td>Finland</td><td>28</td></tr><tr><td>Azerbaijan</td><td>United States</td><td>21</td></tr><tr><td>United States</td><td>Greece</td><td>23</td></tr><tr><td>United States</td><td>The Bahamas</td><td>986</td></tr><tr><td>New Zealand</td><td>United States</td><td>111</td></tr><tr><td>Liberia</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Hong Kong</td><td>414</td></tr><tr><td>Hungary</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>China</td><td>920</td></tr><tr><td>United States</td><td>Vietnam</td><td>2</td></tr><tr><td>Burkina Faso</td><td>United States</td><td>1</td></tr><tr><td>Sweden</td><td>United States</td><td>118</td></tr><tr><td>United States</td><td>Kuwait</td><td>28</td></tr><tr><td>United States</td><td>Dominican Republic</td><td>1420</td></tr><tr><td>United States</td><td>Egypt</td><td>12</td></tr><tr><td>Israel</td><td>United States</td><td>134</td></tr><tr><td>United States</td><td>United States</td><td>370002</td></tr><tr><td>Ethiopia</td><td>United States</td><td>13</td></tr><tr><td>United States</td><td>Luxembourg</td><td>134</td></tr><tr><td>United States</td><td>Poland</td><td>33</td></tr><tr><td>Martinique</td><td>United States</td><td>44</td></tr><tr><td>United States</td><td>Saint Barthelemy</td><td>41</td></tr><tr><td>Saint Barthelemy</td><td>United States</td><td>39</td></tr><tr><td>Barbados</td><td>United States</td><td>154</td></tr><tr><td>United States</td><td>Turkey</td><td>129</td></tr><tr><td>Djibouti</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Azerbaijan</td><td>21</td></tr><tr><td>United States</td><td>Estonia</td><td>1</td></tr><tr><td>Germany</td><td>United States</td><td>1468</td></tr><tr><td>United States</td><td>South Korea</td><td>827</td></tr><tr><td>United States</td><td>El Salvador</td><td>508</td></tr><tr><td>Ireland</td><td>United States</td><td>335</td></tr><tr><td>United States</td><td>Hungary</td><td>3</td></tr><tr><td>Zambia</td><td>United States</td><td>1</td></tr><tr><td>Malaysia</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Ethiopia</td><td>12</td></tr><tr><td>United States</td><td>Panama</td><td>465</td></tr><tr><td>United States</td><td>Aruba</td><td>342</td></tr><tr><td>United States</td><td>Thailand</td><td>4</td></tr><tr><td>United States</td><td>Turks and Caicos Islands</td><td>236</td></tr><tr><td>Croatia</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Pakistan</td><td>12</td></tr><tr><td>Cyprus</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Honduras</td><td>407</td></tr><tr><td>Fiji</td><td>United States</td><td>24</td></tr><tr><td>Qatar</td><td>United States</td><td>108</td></tr><tr><td>Saint Kitts and Nevis</td><td>United States</td><td>139</td></tr><tr><td>Kuwait</td><td>United States</td><td>32</td></tr><tr><td>Taiwan</td><td>United States</td><td>266</td></tr><tr><td>Haiti</td><td>United States</td><td>226</td></tr><tr><td>Canada</td><td>United States</td><td>8399</td></tr><tr><td>Federated States of Micronesia</td><td>United States</td><td>69</td></tr><tr><td>United States</td><td>Liberia</td><td>2</td></tr><tr><td>Jamaica</td><td>United States</td><td>666</td></tr><tr><td>United States</td><td>Malta</td><td>2</td></tr><tr><td>Dominican Republic</td><td>United States</td><td>1353</td></tr><tr><td>Japan</td><td>United States</td><td>1548</td></tr><tr><td>United States</td><td>Lithuania</td><td>1</td></tr><tr><td>Finland</td><td>United States</td><td>26</td></tr><tr><td>United States</td><td>Guadeloupe</td><td>59</td></tr><tr><td>United States</td><td>Ukraine</td><td>13</td></tr><tr><td>United States</td><td>France</td><td>952</td></tr><tr><td>United States</td><td>Norway</td><td>115</td></tr><tr><td>Aruba</td><td>United States</td><td>346</td></tr><tr><td>French Guiana</td><td>United States</td><td>5</td></tr><tr><td>United States</td><td>Kiribati</td><td>35</td></tr><tr><td>India</td><td>United States</td><td>61</td></tr><tr><td>British Virgin Islands</td><td>United States</td><td>107</td></tr><tr><td>Brazil</td><td>United States</td><td>853</td></tr><tr><td>United States</td><td>Germany</td><td>1336</td></tr><tr><td>United States</td><td>New Zealand</td><td>74</td></tr><tr><td>French Polynesia</td><td>United States</td><td>43</td></tr><tr><td>United Arab Emirates</td><td>United States</td><td>320</td></tr><tr><td>Singapore</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>Mexico</td><td>7187</td></tr><tr><td>United States</td><td>Sweden</td><td>119</td></tr><tr><td>Netherlands</td><td>United States</td><td>776</td></tr><tr><td>United States</td><td>Martinique</td><td>43</td></tr><tr><td>United States</td><td>United Arab Emirates</td><td>313</td></tr><tr><td>United States</td><td>Bulgaria</td><td>1</td></tr><tr><td>Denmark</td><td>United States</td><td>153</td></tr><tr><td>China</td><td>United States</td><td>772</td></tr><tr><td>United States</td><td>Nicaragua</td><td>201</td></tr><tr><td>United States</td><td>Philippines</td><td>126</td></tr><tr><td>United States</td><td>Georgia</td><td>1</td></tr><tr><td>United States</td><td>Belgium</td><td>228</td></tr><tr><td>Cayman Islands</td><td>United States</td><td>314</td></tr><tr><td>Argentina</td><td>United States</td><td>180</td></tr><tr><td>Peru</td><td>United States</td><td>279</td></tr><tr><td>South Africa</td><td>United States</td><td>36</td></tr><tr><td>United States</td><td>Iceland</td><td>202</td></tr><tr><td>United States</td><td>Argentina</td><td>141</td></tr><tr><td>Spain</td><td>United States</td><td>420</td></tr><tr><td>Bermuda</td><td>United States</td><td>183</td></tr><tr><td>United States</td><td>Nigeria</td><td>50</td></tr><tr><td>United States</td><td>Austria</td><td>63</td></tr><tr><td>United States</td><td>Bonaire, Sint Eustatius, and Saba</td><td>59</td></tr><tr><td>Kiribati</td><td>United States</td><td>26</td></tr><tr><td>Saudi Arabia</td><td>United States</td><td>83</td></tr><tr><td>Czech Republic</td><td>United States</td><td>13</td></tr><tr><td>United States</td><td>Israel</td><td>127</td></tr><tr><td>Belgium</td><td>United States</td><td>259</td></tr><tr><td>United States</td><td>Saint Lucia</td><td>136</td></tr><tr><td>United States</td><td>Bahrain</td><td>1</td></tr><tr><td>United States</td><td>British Virgin Islands</td><td>80</td></tr><tr><td>Curacao</td><td>United States</td><td>90</td></tr><tr><td>Georgia</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Denmark</td><td>152</td></tr><tr><td>United States</td><td>Guyana</td><td>63</td></tr><tr><td>Philippines</td><td>United States</td><td>134</td></tr><tr><td>Grenada</td><td>United States</td><td>53</td></tr><tr><td>Cape Verde</td><td>United States</td><td>20</td></tr><tr><td>Cote d'Ivoire</td><td>United States</td><td>1</td></tr><tr><td>Ukraine</td><td>United States</td><td>14</td></tr><tr><td>United States</td><td>Papua New Guinea</td><td>1</td></tr><tr><td>Russia</td><td>United States</td><td>176</td></tr><tr><td>United States</td><td>Saudi Arabia</td><td>70</td></tr><tr><td>Guatemala</td><td>United States</td><td>397</td></tr><tr><td>Saint Lucia</td><td>United States</td><td>123</td></tr><tr><td>Paraguay</td><td>United States</td><td>60</td></tr><tr><td>United States</td><td>Curacao</td><td>83</td></tr><tr><td>Kosovo</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Taiwan</td><td>235</td></tr><tr><td>Tunisia</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>South Africa</td><td>40</td></tr><tr><td>Niger</td><td>United States</td><td>2</td></tr><tr><td>Turkey</td><td>United States</td><td>138</td></tr><tr><td>United Kingdom</td><td>United States</td><td>2025</td></tr><tr><td>Romania</td><td>United States</td><td>14</td></tr><tr><td>United States</td><td>Greenland</td><td>4</td></tr><tr><td>Papua New Guinea</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>Spain</td><td>442</td></tr><tr><td>Iraq</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Italy</td><td>438</td></tr><tr><td>Cuba</td><td>United States</td><td>466</td></tr><tr><td>United States</td><td>Switzerland</td><td>305</td></tr><tr><td>Dominica</td><td>United States</td><td>20</td></tr><tr><td>United States</td><td>Japan</td><td>1496</td></tr><tr><td>Portugal</td><td>United States</td><td>127</td></tr><tr><td>United States</td><td>Brazil</td><td>619</td></tr><tr><td>Bahrain</td><td>United States</td><td>19</td></tr><tr><td>United States</td><td>Peru</td><td>337</td></tr><tr><td>Indonesia</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Belize</td><td>193</td></tr><tr><td>United States</td><td>United Kingdom</td><td>1970</td></tr><tr><td>Belize</td><td>United States</td><td>188</td></tr><tr><td>United States</td><td>Ghana</td><td>20</td></tr><tr><td>United States</td><td>Indonesia</td><td>2</td></tr><tr><td>United States</td><td>Fiji</td><td>25</td></tr><tr><td>United States</td><td>Canada</td><td>8483</td></tr><tr><td>United States</td><td>Antigua and Barbuda</td><td>117</td></tr><tr><td>United States</td><td>French Polynesia</td><td>40</td></tr><tr><td>Nicaragua</td><td>United States</td><td>179</td></tr><tr><td>United States</td><td>Latvia</td><td>15</td></tr><tr><td>United States</td><td>Dominica</td><td>27</td></tr><tr><td>United States</td><td>Czech Republic</td><td>12</td></tr><tr><td>United States</td><td>Australia</td><td>258</td></tr><tr><td>United States</td><td>Cook Islands</td><td>13</td></tr><tr><td>Austria</td><td>United States</td><td>62</td></tr><tr><td>Jordan</td><td>United States</td><td>44</td></tr><tr><td>Palau</td><td>United States</td><td>30</td></tr><tr><td>South Korea</td><td>United States</td><td>1048</td></tr><tr><td>Angola</td><td>United States</td><td>15</td></tr><tr><td>Ghana</td><td>United States</td><td>18</td></tr><tr><td>New Caledonia</td><td>United States</td><td>1</td></tr><tr><td>Guadeloupe</td><td>United States</td><td>56</td></tr><tr><td>France</td><td>United States</td><td>935</td></tr><tr><td>Poland</td><td>United States</td><td>32</td></tr><tr><td>Nigeria</td><td>United States</td><td>59</td></tr><tr><td>United States</td><td>Uruguay</td><td>13</td></tr><tr><td>Greenland</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Bermuda</td><td>193</td></tr><tr><td>Chile</td><td>United States</td><td>174</td></tr><tr><td>United States</td><td>Cuba</td><td>478</td></tr><tr><td>United States</td><td>Montenegro</td><td>1</td></tr><tr><td>United States</td><td>Colombia</td><td>867</td></tr><tr><td>United States</td><td>Barbados</td><td>130</td></tr><tr><td>United States</td><td>Qatar</td><td>109</td></tr><tr><td>Australia</td><td>United States</td><td>329</td></tr><tr><td>United States</td><td>Cayman Islands</td><td>310</td></tr><tr><td>United States</td><td>Jordan</td><td>44</td></tr><tr><td>United States</td><td>Namibia</td><td>1</td></tr><tr><td>United States</td><td>Trinidad and Tobago</td><td>217</td></tr><tr><td>United States</td><td>Bolivia</td><td>13</td></tr><tr><td>Cook Islands</td><td>United States</td><td>13</td></tr><tr><td>Bulgaria</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>Saint Kitts and Nevis</td><td>145</td></tr><tr><td>Uruguay</td><td>United States</td><td>43</td></tr><tr><td>United States</td><td>Haiti</td><td>225</td></tr><tr><td>Bonaire, Sint Eustatius, and Saba</td><td>United States</td><td>58</td></tr><tr><td>Greece</td><td>United States</td><td>30</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DEST_COUNTRY_NAME",
         "ORIGIN_COUNTRY_NAME",
         "count"
        ],
        [
         "United States",
         "Romania",
         "15"
        ],
        [
         "United States",
         "Croatia",
         "1"
        ],
        [
         "United States",
         "Ireland",
         "344"
        ],
        [
         "Egypt",
         "United States",
         "15"
        ],
        [
         "United States",
         "India",
         "62"
        ],
        [
         "United States",
         "Singapore",
         "1"
        ],
        [
         "United States",
         "Grenada",
         "62"
        ],
        [
         "Costa Rica",
         "United States",
         "588"
        ],
        [
         "Senegal",
         "United States",
         "40"
        ],
        [
         "Moldova",
         "United States",
         "1"
        ],
        [
         "United States",
         "Sint Maarten",
         "325"
        ],
        [
         "United States",
         "Marshall Islands",
         "39"
        ],
        [
         "Guyana",
         "United States",
         "64"
        ],
        [
         "Malta",
         "United States",
         "1"
        ],
        [
         "Anguilla",
         "United States",
         "41"
        ],
        [
         "Bolivia",
         "United States",
         "30"
        ],
        [
         "United States",
         "Paraguay",
         "6"
        ],
        [
         "Algeria",
         "United States",
         "4"
        ],
        [
         "Turks and Caicos Islands",
         "United States",
         "230"
        ],
        [
         "United States",
         "Gibraltar",
         "1"
        ],
        [
         "Saint Vincent and the Grenadines",
         "United States",
         "1"
        ],
        [
         "Italy",
         "United States",
         "382"
        ],
        [
         "United States",
         "Federated States of Micronesia",
         "69"
        ],
        [
         "United States",
         "Russia",
         "161"
        ],
        [
         "Pakistan",
         "United States",
         "12"
        ],
        [
         "United States",
         "Netherlands",
         "660"
        ],
        [
         "Iceland",
         "United States",
         "181"
        ],
        [
         "Marshall Islands",
         "United States",
         "42"
        ],
        [
         "Luxembourg",
         "United States",
         "155"
        ],
        [
         "Honduras",
         "United States",
         "362"
        ],
        [
         "The Bahamas",
         "United States",
         "955"
        ],
        [
         "United States",
         "Senegal",
         "42"
        ],
        [
         "El Salvador",
         "United States",
         "561"
        ],
        [
         "Samoa",
         "United States",
         "25"
        ],
        [
         "United States",
         "Angola",
         "13"
        ],
        [
         "Switzerland",
         "United States",
         "294"
        ],
        [
         "United States",
         "Anguilla",
         "38"
        ],
        [
         "Sint Maarten",
         "United States",
         "325"
        ],
        [
         "Hong Kong",
         "United States",
         "332"
        ],
        [
         "Trinidad and Tobago",
         "United States",
         "211"
        ],
        [
         "Latvia",
         "United States",
         "19"
        ],
        [
         "United States",
         "Ecuador",
         "300"
        ],
        [
         "Suriname",
         "United States",
         "1"
        ],
        [
         "Mexico",
         "United States",
         "7140"
        ],
        [
         "United States",
         "Cyprus",
         "1"
        ],
        [
         "Ecuador",
         "United States",
         "268"
        ],
        [
         "United States",
         "Portugal",
         "134"
        ],
        [
         "United States",
         "Costa Rica",
         "608"
        ],
        [
         "United States",
         "Guatemala",
         "318"
        ],
        [
         "United States",
         "Suriname",
         "34"
        ],
        [
         "Colombia",
         "United States",
         "873"
        ],
        [
         "United States",
         "Cape Verde",
         "14"
        ],
        [
         "United States",
         "Jamaica",
         "712"
        ],
        [
         "Norway",
         "United States",
         "121"
        ],
        [
         "United States",
         "Malaysia",
         "3"
        ],
        [
         "United States",
         "Morocco",
         "19"
        ],
        [
         "Thailand",
         "United States",
         "3"
        ],
        [
         "United States",
         "Samoa",
         "25"
        ],
        [
         "Venezuela",
         "United States",
         "290"
        ],
        [
         "United States",
         "Palau",
         "31"
        ],
        [
         "United States",
         "Venezuela",
         "246"
        ],
        [
         "Panama",
         "United States",
         "510"
        ],
        [
         "Antigua and Barbuda",
         "United States",
         "126"
        ],
        [
         "United States",
         "Chile",
         "185"
        ],
        [
         "Morocco",
         "United States",
         "15"
        ],
        [
         "United States",
         "Finland",
         "28"
        ],
        [
         "Azerbaijan",
         "United States",
         "21"
        ],
        [
         "United States",
         "Greece",
         "23"
        ],
        [
         "United States",
         "The Bahamas",
         "986"
        ],
        [
         "New Zealand",
         "United States",
         "111"
        ],
        [
         "Liberia",
         "United States",
         "2"
        ],
        [
         "United States",
         "Hong Kong",
         "414"
        ],
        [
         "Hungary",
         "United States",
         "2"
        ],
        [
         "United States",
         "China",
         "920"
        ],
        [
         "United States",
         "Vietnam",
         "2"
        ],
        [
         "Burkina Faso",
         "United States",
         "1"
        ],
        [
         "Sweden",
         "United States",
         "118"
        ],
        [
         "United States",
         "Kuwait",
         "28"
        ],
        [
         "United States",
         "Dominican Republic",
         "1420"
        ],
        [
         "United States",
         "Egypt",
         "12"
        ],
        [
         "Israel",
         "United States",
         "134"
        ],
        [
         "United States",
         "United States",
         "370002"
        ],
        [
         "Ethiopia",
         "United States",
         "13"
        ],
        [
         "United States",
         "Luxembourg",
         "134"
        ],
        [
         "United States",
         "Poland",
         "33"
        ],
        [
         "Martinique",
         "United States",
         "44"
        ],
        [
         "United States",
         "Saint Barthelemy",
         "41"
        ],
        [
         "Saint Barthelemy",
         "United States",
         "39"
        ],
        [
         "Barbados",
         "United States",
         "154"
        ],
        [
         "United States",
         "Turkey",
         "129"
        ],
        [
         "Djibouti",
         "United States",
         "1"
        ],
        [
         "United States",
         "Azerbaijan",
         "21"
        ],
        [
         "United States",
         "Estonia",
         "1"
        ],
        [
         "Germany",
         "United States",
         "1468"
        ],
        [
         "United States",
         "South Korea",
         "827"
        ],
        [
         "United States",
         "El Salvador",
         "508"
        ],
        [
         "Ireland",
         "United States",
         "335"
        ],
        [
         "United States",
         "Hungary",
         "3"
        ],
        [
         "Zambia",
         "United States",
         "1"
        ],
        [
         "Malaysia",
         "United States",
         "2"
        ],
        [
         "United States",
         "Ethiopia",
         "12"
        ],
        [
         "United States",
         "Panama",
         "465"
        ],
        [
         "United States",
         "Aruba",
         "342"
        ],
        [
         "United States",
         "Thailand",
         "4"
        ],
        [
         "United States",
         "Turks and Caicos Islands",
         "236"
        ],
        [
         "Croatia",
         "United States",
         "2"
        ],
        [
         "United States",
         "Pakistan",
         "12"
        ],
        [
         "Cyprus",
         "United States",
         "1"
        ],
        [
         "United States",
         "Honduras",
         "407"
        ],
        [
         "Fiji",
         "United States",
         "24"
        ],
        [
         "Qatar",
         "United States",
         "108"
        ],
        [
         "Saint Kitts and Nevis",
         "United States",
         "139"
        ],
        [
         "Kuwait",
         "United States",
         "32"
        ],
        [
         "Taiwan",
         "United States",
         "266"
        ],
        [
         "Haiti",
         "United States",
         "226"
        ],
        [
         "Canada",
         "United States",
         "8399"
        ],
        [
         "Federated States of Micronesia",
         "United States",
         "69"
        ],
        [
         "United States",
         "Liberia",
         "2"
        ],
        [
         "Jamaica",
         "United States",
         "666"
        ],
        [
         "United States",
         "Malta",
         "2"
        ],
        [
         "Dominican Republic",
         "United States",
         "1353"
        ],
        [
         "Japan",
         "United States",
         "1548"
        ],
        [
         "United States",
         "Lithuania",
         "1"
        ],
        [
         "Finland",
         "United States",
         "26"
        ],
        [
         "United States",
         "Guadeloupe",
         "59"
        ],
        [
         "United States",
         "Ukraine",
         "13"
        ],
        [
         "United States",
         "France",
         "952"
        ],
        [
         "United States",
         "Norway",
         "115"
        ],
        [
         "Aruba",
         "United States",
         "346"
        ],
        [
         "French Guiana",
         "United States",
         "5"
        ],
        [
         "United States",
         "Kiribati",
         "35"
        ],
        [
         "India",
         "United States",
         "61"
        ],
        [
         "British Virgin Islands",
         "United States",
         "107"
        ],
        [
         "Brazil",
         "United States",
         "853"
        ],
        [
         "United States",
         "Germany",
         "1336"
        ],
        [
         "United States",
         "New Zealand",
         "74"
        ],
        [
         "French Polynesia",
         "United States",
         "43"
        ],
        [
         "United Arab Emirates",
         "United States",
         "320"
        ],
        [
         "Singapore",
         "United States",
         "3"
        ],
        [
         "United States",
         "Mexico",
         "7187"
        ],
        [
         "United States",
         "Sweden",
         "119"
        ],
        [
         "Netherlands",
         "United States",
         "776"
        ],
        [
         "United States",
         "Martinique",
         "43"
        ],
        [
         "United States",
         "United Arab Emirates",
         "313"
        ],
        [
         "United States",
         "Bulgaria",
         "1"
        ],
        [
         "Denmark",
         "United States",
         "153"
        ],
        [
         "China",
         "United States",
         "772"
        ],
        [
         "United States",
         "Nicaragua",
         "201"
        ],
        [
         "United States",
         "Philippines",
         "126"
        ],
        [
         "United States",
         "Georgia",
         "1"
        ],
        [
         "United States",
         "Belgium",
         "228"
        ],
        [
         "Cayman Islands",
         "United States",
         "314"
        ],
        [
         "Argentina",
         "United States",
         "180"
        ],
        [
         "Peru",
         "United States",
         "279"
        ],
        [
         "South Africa",
         "United States",
         "36"
        ],
        [
         "United States",
         "Iceland",
         "202"
        ],
        [
         "United States",
         "Argentina",
         "141"
        ],
        [
         "Spain",
         "United States",
         "420"
        ],
        [
         "Bermuda",
         "United States",
         "183"
        ],
        [
         "United States",
         "Nigeria",
         "50"
        ],
        [
         "United States",
         "Austria",
         "63"
        ],
        [
         "United States",
         "Bonaire, Sint Eustatius, and Saba",
         "59"
        ],
        [
         "Kiribati",
         "United States",
         "26"
        ],
        [
         "Saudi Arabia",
         "United States",
         "83"
        ],
        [
         "Czech Republic",
         "United States",
         "13"
        ],
        [
         "United States",
         "Israel",
         "127"
        ],
        [
         "Belgium",
         "United States",
         "259"
        ],
        [
         "United States",
         "Saint Lucia",
         "136"
        ],
        [
         "United States",
         "Bahrain",
         "1"
        ],
        [
         "United States",
         "British Virgin Islands",
         "80"
        ],
        [
         "Curacao",
         "United States",
         "90"
        ],
        [
         "Georgia",
         "United States",
         "2"
        ],
        [
         "United States",
         "Denmark",
         "152"
        ],
        [
         "United States",
         "Guyana",
         "63"
        ],
        [
         "Philippines",
         "United States",
         "134"
        ],
        [
         "Grenada",
         "United States",
         "53"
        ],
        [
         "Cape Verde",
         "United States",
         "20"
        ],
        [
         "Cote d'Ivoire",
         "United States",
         "1"
        ],
        [
         "Ukraine",
         "United States",
         "14"
        ],
        [
         "United States",
         "Papua New Guinea",
         "1"
        ],
        [
         "Russia",
         "United States",
         "176"
        ],
        [
         "United States",
         "Saudi Arabia",
         "70"
        ],
        [
         "Guatemala",
         "United States",
         "397"
        ],
        [
         "Saint Lucia",
         "United States",
         "123"
        ],
        [
         "Paraguay",
         "United States",
         "60"
        ],
        [
         "United States",
         "Curacao",
         "83"
        ],
        [
         "Kosovo",
         "United States",
         "1"
        ],
        [
         "United States",
         "Taiwan",
         "235"
        ],
        [
         "Tunisia",
         "United States",
         "3"
        ],
        [
         "United States",
         "South Africa",
         "40"
        ],
        [
         "Niger",
         "United States",
         "2"
        ],
        [
         "Turkey",
         "United States",
         "138"
        ],
        [
         "United Kingdom",
         "United States",
         "2025"
        ],
        [
         "Romania",
         "United States",
         "14"
        ],
        [
         "United States",
         "Greenland",
         "4"
        ],
        [
         "Papua New Guinea",
         "United States",
         "3"
        ],
        [
         "United States",
         "Spain",
         "442"
        ],
        [
         "Iraq",
         "United States",
         "1"
        ],
        [
         "United States",
         "Italy",
         "438"
        ],
        [
         "Cuba",
         "United States",
         "466"
        ],
        [
         "United States",
         "Switzerland",
         "305"
        ],
        [
         "Dominica",
         "United States",
         "20"
        ],
        [
         "United States",
         "Japan",
         "1496"
        ],
        [
         "Portugal",
         "United States",
         "127"
        ],
        [
         "United States",
         "Brazil",
         "619"
        ],
        [
         "Bahrain",
         "United States",
         "19"
        ],
        [
         "United States",
         "Peru",
         "337"
        ],
        [
         "Indonesia",
         "United States",
         "1"
        ],
        [
         "United States",
         "Belize",
         "193"
        ],
        [
         "United States",
         "United Kingdom",
         "1970"
        ],
        [
         "Belize",
         "United States",
         "188"
        ],
        [
         "United States",
         "Ghana",
         "20"
        ],
        [
         "United States",
         "Indonesia",
         "2"
        ],
        [
         "United States",
         "Fiji",
         "25"
        ],
        [
         "United States",
         "Canada",
         "8483"
        ],
        [
         "United States",
         "Antigua and Barbuda",
         "117"
        ],
        [
         "United States",
         "French Polynesia",
         "40"
        ],
        [
         "Nicaragua",
         "United States",
         "179"
        ],
        [
         "United States",
         "Latvia",
         "15"
        ],
        [
         "United States",
         "Dominica",
         "27"
        ],
        [
         "United States",
         "Czech Republic",
         "12"
        ],
        [
         "United States",
         "Australia",
         "258"
        ],
        [
         "United States",
         "Cook Islands",
         "13"
        ],
        [
         "Austria",
         "United States",
         "62"
        ],
        [
         "Jordan",
         "United States",
         "44"
        ],
        [
         "Palau",
         "United States",
         "30"
        ],
        [
         "South Korea",
         "United States",
         "1048"
        ],
        [
         "Angola",
         "United States",
         "15"
        ],
        [
         "Ghana",
         "United States",
         "18"
        ],
        [
         "New Caledonia",
         "United States",
         "1"
        ],
        [
         "Guadeloupe",
         "United States",
         "56"
        ],
        [
         "France",
         "United States",
         "935"
        ],
        [
         "Poland",
         "United States",
         "32"
        ],
        [
         "Nigeria",
         "United States",
         "59"
        ],
        [
         "United States",
         "Uruguay",
         "13"
        ],
        [
         "Greenland",
         "United States",
         "2"
        ],
        [
         "United States",
         "Bermuda",
         "193"
        ],
        [
         "Chile",
         "United States",
         "174"
        ],
        [
         "United States",
         "Cuba",
         "478"
        ],
        [
         "United States",
         "Montenegro",
         "1"
        ],
        [
         "United States",
         "Colombia",
         "867"
        ],
        [
         "United States",
         "Barbados",
         "130"
        ],
        [
         "United States",
         "Qatar",
         "109"
        ],
        [
         "Australia",
         "United States",
         "329"
        ],
        [
         "United States",
         "Cayman Islands",
         "310"
        ],
        [
         "United States",
         "Jordan",
         "44"
        ],
        [
         "United States",
         "Namibia",
         "1"
        ],
        [
         "United States",
         "Trinidad and Tobago",
         "217"
        ],
        [
         "United States",
         "Bolivia",
         "13"
        ],
        [
         "Cook Islands",
         "United States",
         "13"
        ],
        [
         "Bulgaria",
         "United States",
         "3"
        ],
        [
         "United States",
         "Saint Kitts and Nevis",
         "145"
        ],
        [
         "Uruguay",
         "United States",
         "43"
        ],
        [
         "United States",
         "Haiti",
         "225"
        ],
        [
         "Bonaire, Sint Eustatius, and Saba",
         "United States",
         "58"
        ],
        [
         "Greece",
         "United States",
         "30"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eaa4dc4-dfb0-443a-b877-6147b7e08fc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"true\")\\\n",
    "# Setting read options\n",
    "    .option(\"inferschema\",\"false\")\\\n",
    "# Setting read options\n",
    "        .option(\"mode\",\"FAILFAST\")\\\n",
    "# Loading the data from specified path\n",
    "        .load(\"/FileStore/tables/2015_summary.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9450ae83-d039-4008-abcb-0b6c3e655305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr></thead><tbody><tr><td>United States</td><td>Romania</td><td>15</td></tr><tr><td>United States</td><td>Croatia</td><td>1</td></tr><tr><td>United States</td><td>Ireland</td><td>344</td></tr><tr><td>Egypt</td><td>United States</td><td>15</td></tr><tr><td>United States</td><td>India</td><td>62</td></tr><tr><td>United States</td><td>Singapore</td><td>1</td></tr><tr><td>United States</td><td>Grenada</td><td>62</td></tr><tr><td>Costa Rica</td><td>United States</td><td>588</td></tr><tr><td>Senegal</td><td>United States</td><td>40</td></tr><tr><td>Moldova</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Sint Maarten</td><td>325</td></tr><tr><td>United States</td><td>Marshall Islands</td><td>39</td></tr><tr><td>Guyana</td><td>United States</td><td>64</td></tr><tr><td>Malta</td><td>United States</td><td>1</td></tr><tr><td>Anguilla</td><td>United States</td><td>41</td></tr><tr><td>Bolivia</td><td>United States</td><td>30</td></tr><tr><td>United States</td><td>Paraguay</td><td>6</td></tr><tr><td>Algeria</td><td>United States</td><td>4</td></tr><tr><td>Turks and Caicos Islands</td><td>United States</td><td>230</td></tr><tr><td>United States</td><td>Gibraltar</td><td>1</td></tr><tr><td>Saint Vincent and the Grenadines</td><td>United States</td><td>1</td></tr><tr><td>Italy</td><td>United States</td><td>382</td></tr><tr><td>United States</td><td>Federated States of Micronesia</td><td>69</td></tr><tr><td>United States</td><td>Russia</td><td>161</td></tr><tr><td>Pakistan</td><td>United States</td><td>12</td></tr><tr><td>United States</td><td>Netherlands</td><td>660</td></tr><tr><td>Iceland</td><td>United States</td><td>181</td></tr><tr><td>Marshall Islands</td><td>United States</td><td>42</td></tr><tr><td>Luxembourg</td><td>United States</td><td>155</td></tr><tr><td>Honduras</td><td>United States</td><td>362</td></tr><tr><td>The Bahamas</td><td>United States</td><td>955</td></tr><tr><td>United States</td><td>Senegal</td><td>42</td></tr><tr><td>El Salvador</td><td>United States</td><td>561</td></tr><tr><td>Samoa</td><td>United States</td><td>25</td></tr><tr><td>United States</td><td>Angola</td><td>13</td></tr><tr><td>Switzerland</td><td>United States</td><td>294</td></tr><tr><td>United States</td><td>Anguilla</td><td>38</td></tr><tr><td>Sint Maarten</td><td>United States</td><td>325</td></tr><tr><td>Hong Kong</td><td>United States</td><td>332</td></tr><tr><td>Trinidad and Tobago</td><td>United States</td><td>211</td></tr><tr><td>Latvia</td><td>United States</td><td>19</td></tr><tr><td>United States</td><td>Ecuador</td><td>300</td></tr><tr><td>Suriname</td><td>United States</td><td>1</td></tr><tr><td>Mexico</td><td>United States</td><td>7140</td></tr><tr><td>United States</td><td>Cyprus</td><td>1</td></tr><tr><td>Ecuador</td><td>United States</td><td>268</td></tr><tr><td>United States</td><td>Portugal</td><td>134</td></tr><tr><td>United States</td><td>Costa Rica</td><td>608</td></tr><tr><td>United States</td><td>Guatemala</td><td>318</td></tr><tr><td>United States</td><td>Suriname</td><td>34</td></tr><tr><td>Colombia</td><td>United States</td><td>873</td></tr><tr><td>United States</td><td>Cape Verde</td><td>14</td></tr><tr><td>United States</td><td>Jamaica</td><td>712</td></tr><tr><td>Norway</td><td>United States</td><td>121</td></tr><tr><td>United States</td><td>Malaysia</td><td>3</td></tr><tr><td>United States</td><td>Morocco</td><td>19</td></tr><tr><td>Thailand</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>Samoa</td><td>25</td></tr><tr><td>Venezuela</td><td>United States</td><td>290</td></tr><tr><td>United States</td><td>Palau</td><td>31</td></tr><tr><td>United States</td><td>Venezuela</td><td>246</td></tr><tr><td>Panama</td><td>United States</td><td>510</td></tr><tr><td>Antigua and Barbuda</td><td>United States</td><td>126</td></tr><tr><td>United States</td><td>Chile</td><td>185</td></tr><tr><td>Morocco</td><td>United States</td><td>15</td></tr><tr><td>United States</td><td>Finland</td><td>28</td></tr><tr><td>Azerbaijan</td><td>United States</td><td>21</td></tr><tr><td>United States</td><td>Greece</td><td>23</td></tr><tr><td>United States</td><td>The Bahamas</td><td>986</td></tr><tr><td>New Zealand</td><td>United States</td><td>111</td></tr><tr><td>Liberia</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Hong Kong</td><td>414</td></tr><tr><td>Hungary</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>China</td><td>920</td></tr><tr><td>United States</td><td>Vietnam</td><td>2</td></tr><tr><td>Burkina Faso</td><td>United States</td><td>1</td></tr><tr><td>Sweden</td><td>United States</td><td>118</td></tr><tr><td>United States</td><td>Kuwait</td><td>28</td></tr><tr><td>United States</td><td>Dominican Republic</td><td>1420</td></tr><tr><td>United States</td><td>Egypt</td><td>12</td></tr><tr><td>Israel</td><td>United States</td><td>134</td></tr><tr><td>United States</td><td>United States</td><td>370002</td></tr><tr><td>Ethiopia</td><td>United States</td><td>13</td></tr><tr><td>United States</td><td>Luxembourg</td><td>134</td></tr><tr><td>United States</td><td>Poland</td><td>33</td></tr><tr><td>Martinique</td><td>United States</td><td>44</td></tr><tr><td>United States</td><td>Saint Barthelemy</td><td>41</td></tr><tr><td>Saint Barthelemy</td><td>United States</td><td>39</td></tr><tr><td>Barbados</td><td>United States</td><td>154</td></tr><tr><td>United States</td><td>Turkey</td><td>129</td></tr><tr><td>Djibouti</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Azerbaijan</td><td>21</td></tr><tr><td>United States</td><td>Estonia</td><td>1</td></tr><tr><td>Germany</td><td>United States</td><td>1468</td></tr><tr><td>United States</td><td>South Korea</td><td>827</td></tr><tr><td>United States</td><td>El Salvador</td><td>508</td></tr><tr><td>Ireland</td><td>United States</td><td>335</td></tr><tr><td>United States</td><td>Hungary</td><td>3</td></tr><tr><td>Zambia</td><td>United States</td><td>1</td></tr><tr><td>Malaysia</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Ethiopia</td><td>12</td></tr><tr><td>United States</td><td>Panama</td><td>465</td></tr><tr><td>United States</td><td>Aruba</td><td>342</td></tr><tr><td>United States</td><td>Thailand</td><td>4</td></tr><tr><td>United States</td><td>Turks and Caicos Islands</td><td>236</td></tr><tr><td>Croatia</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Pakistan</td><td>12</td></tr><tr><td>Cyprus</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Honduras</td><td>407</td></tr><tr><td>Fiji</td><td>United States</td><td>24</td></tr><tr><td>Qatar</td><td>United States</td><td>108</td></tr><tr><td>Saint Kitts and Nevis</td><td>United States</td><td>139</td></tr><tr><td>Kuwait</td><td>United States</td><td>32</td></tr><tr><td>Taiwan</td><td>United States</td><td>266</td></tr><tr><td>Haiti</td><td>United States</td><td>226</td></tr><tr><td>Canada</td><td>United States</td><td>8399</td></tr><tr><td>Federated States of Micronesia</td><td>United States</td><td>69</td></tr><tr><td>United States</td><td>Liberia</td><td>2</td></tr><tr><td>Jamaica</td><td>United States</td><td>666</td></tr><tr><td>United States</td><td>Malta</td><td>2</td></tr><tr><td>Dominican Republic</td><td>United States</td><td>1353</td></tr><tr><td>Japan</td><td>United States</td><td>1548</td></tr><tr><td>United States</td><td>Lithuania</td><td>1</td></tr><tr><td>Finland</td><td>United States</td><td>26</td></tr><tr><td>United States</td><td>Guadeloupe</td><td>59</td></tr><tr><td>United States</td><td>Ukraine</td><td>13</td></tr><tr><td>United States</td><td>France</td><td>952</td></tr><tr><td>United States</td><td>Norway</td><td>115</td></tr><tr><td>Aruba</td><td>United States</td><td>346</td></tr><tr><td>French Guiana</td><td>United States</td><td>5</td></tr><tr><td>United States</td><td>Kiribati</td><td>35</td></tr><tr><td>India</td><td>United States</td><td>61</td></tr><tr><td>British Virgin Islands</td><td>United States</td><td>107</td></tr><tr><td>Brazil</td><td>United States</td><td>853</td></tr><tr><td>United States</td><td>Germany</td><td>1336</td></tr><tr><td>United States</td><td>New Zealand</td><td>74</td></tr><tr><td>French Polynesia</td><td>United States</td><td>43</td></tr><tr><td>United Arab Emirates</td><td>United States</td><td>320</td></tr><tr><td>Singapore</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>Mexico</td><td>7187</td></tr><tr><td>United States</td><td>Sweden</td><td>119</td></tr><tr><td>Netherlands</td><td>United States</td><td>776</td></tr><tr><td>United States</td><td>Martinique</td><td>43</td></tr><tr><td>United States</td><td>United Arab Emirates</td><td>313</td></tr><tr><td>United States</td><td>Bulgaria</td><td>1</td></tr><tr><td>Denmark</td><td>United States</td><td>153</td></tr><tr><td>China</td><td>United States</td><td>772</td></tr><tr><td>United States</td><td>Nicaragua</td><td>201</td></tr><tr><td>United States</td><td>Philippines</td><td>126</td></tr><tr><td>United States</td><td>Georgia</td><td>1</td></tr><tr><td>United States</td><td>Belgium</td><td>228</td></tr><tr><td>Cayman Islands</td><td>United States</td><td>314</td></tr><tr><td>Argentina</td><td>United States</td><td>180</td></tr><tr><td>Peru</td><td>United States</td><td>279</td></tr><tr><td>South Africa</td><td>United States</td><td>36</td></tr><tr><td>United States</td><td>Iceland</td><td>202</td></tr><tr><td>United States</td><td>Argentina</td><td>141</td></tr><tr><td>Spain</td><td>United States</td><td>420</td></tr><tr><td>Bermuda</td><td>United States</td><td>183</td></tr><tr><td>United States</td><td>Nigeria</td><td>50</td></tr><tr><td>United States</td><td>Austria</td><td>63</td></tr><tr><td>United States</td><td>Bonaire, Sint Eustatius, and Saba</td><td>59</td></tr><tr><td>Kiribati</td><td>United States</td><td>26</td></tr><tr><td>Saudi Arabia</td><td>United States</td><td>83</td></tr><tr><td>Czech Republic</td><td>United States</td><td>13</td></tr><tr><td>United States</td><td>Israel</td><td>127</td></tr><tr><td>Belgium</td><td>United States</td><td>259</td></tr><tr><td>United States</td><td>Saint Lucia</td><td>136</td></tr><tr><td>United States</td><td>Bahrain</td><td>1</td></tr><tr><td>United States</td><td>British Virgin Islands</td><td>80</td></tr><tr><td>Curacao</td><td>United States</td><td>90</td></tr><tr><td>Georgia</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Denmark</td><td>152</td></tr><tr><td>United States</td><td>Guyana</td><td>63</td></tr><tr><td>Philippines</td><td>United States</td><td>134</td></tr><tr><td>Grenada</td><td>United States</td><td>53</td></tr><tr><td>Cape Verde</td><td>United States</td><td>20</td></tr><tr><td>Cote d'Ivoire</td><td>United States</td><td>1</td></tr><tr><td>Ukraine</td><td>United States</td><td>14</td></tr><tr><td>United States</td><td>Papua New Guinea</td><td>1</td></tr><tr><td>Russia</td><td>United States</td><td>176</td></tr><tr><td>United States</td><td>Saudi Arabia</td><td>70</td></tr><tr><td>Guatemala</td><td>United States</td><td>397</td></tr><tr><td>Saint Lucia</td><td>United States</td><td>123</td></tr><tr><td>Paraguay</td><td>United States</td><td>60</td></tr><tr><td>United States</td><td>Curacao</td><td>83</td></tr><tr><td>Kosovo</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Taiwan</td><td>235</td></tr><tr><td>Tunisia</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>South Africa</td><td>40</td></tr><tr><td>Niger</td><td>United States</td><td>2</td></tr><tr><td>Turkey</td><td>United States</td><td>138</td></tr><tr><td>United Kingdom</td><td>United States</td><td>2025</td></tr><tr><td>Romania</td><td>United States</td><td>14</td></tr><tr><td>United States</td><td>Greenland</td><td>4</td></tr><tr><td>Papua New Guinea</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>Spain</td><td>442</td></tr><tr><td>Iraq</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Italy</td><td>438</td></tr><tr><td>Cuba</td><td>United States</td><td>466</td></tr><tr><td>United States</td><td>Switzerland</td><td>305</td></tr><tr><td>Dominica</td><td>United States</td><td>20</td></tr><tr><td>United States</td><td>Japan</td><td>1496</td></tr><tr><td>Portugal</td><td>United States</td><td>127</td></tr><tr><td>United States</td><td>Brazil</td><td>619</td></tr><tr><td>Bahrain</td><td>United States</td><td>19</td></tr><tr><td>United States</td><td>Peru</td><td>337</td></tr><tr><td>Indonesia</td><td>United States</td><td>1</td></tr><tr><td>United States</td><td>Belize</td><td>193</td></tr><tr><td>United States</td><td>United Kingdom</td><td>1970</td></tr><tr><td>Belize</td><td>United States</td><td>188</td></tr><tr><td>United States</td><td>Ghana</td><td>20</td></tr><tr><td>United States</td><td>Indonesia</td><td>2</td></tr><tr><td>United States</td><td>Fiji</td><td>25</td></tr><tr><td>United States</td><td>Canada</td><td>8483</td></tr><tr><td>United States</td><td>Antigua and Barbuda</td><td>117</td></tr><tr><td>United States</td><td>French Polynesia</td><td>40</td></tr><tr><td>Nicaragua</td><td>United States</td><td>179</td></tr><tr><td>United States</td><td>Latvia</td><td>15</td></tr><tr><td>United States</td><td>Dominica</td><td>27</td></tr><tr><td>United States</td><td>Czech Republic</td><td>12</td></tr><tr><td>United States</td><td>Australia</td><td>258</td></tr><tr><td>United States</td><td>Cook Islands</td><td>13</td></tr><tr><td>Austria</td><td>United States</td><td>62</td></tr><tr><td>Jordan</td><td>United States</td><td>44</td></tr><tr><td>Palau</td><td>United States</td><td>30</td></tr><tr><td>South Korea</td><td>United States</td><td>1048</td></tr><tr><td>Angola</td><td>United States</td><td>15</td></tr><tr><td>Ghana</td><td>United States</td><td>18</td></tr><tr><td>New Caledonia</td><td>United States</td><td>1</td></tr><tr><td>Guadeloupe</td><td>United States</td><td>56</td></tr><tr><td>France</td><td>United States</td><td>935</td></tr><tr><td>Poland</td><td>United States</td><td>32</td></tr><tr><td>Nigeria</td><td>United States</td><td>59</td></tr><tr><td>United States</td><td>Uruguay</td><td>13</td></tr><tr><td>Greenland</td><td>United States</td><td>2</td></tr><tr><td>United States</td><td>Bermuda</td><td>193</td></tr><tr><td>Chile</td><td>United States</td><td>174</td></tr><tr><td>United States</td><td>Cuba</td><td>478</td></tr><tr><td>United States</td><td>Montenegro</td><td>1</td></tr><tr><td>United States</td><td>Colombia</td><td>867</td></tr><tr><td>United States</td><td>Barbados</td><td>130</td></tr><tr><td>United States</td><td>Qatar</td><td>109</td></tr><tr><td>Australia</td><td>United States</td><td>329</td></tr><tr><td>United States</td><td>Cayman Islands</td><td>310</td></tr><tr><td>United States</td><td>Jordan</td><td>44</td></tr><tr><td>United States</td><td>Namibia</td><td>1</td></tr><tr><td>United States</td><td>Trinidad and Tobago</td><td>217</td></tr><tr><td>United States</td><td>Bolivia</td><td>13</td></tr><tr><td>Cook Islands</td><td>United States</td><td>13</td></tr><tr><td>Bulgaria</td><td>United States</td><td>3</td></tr><tr><td>United States</td><td>Saint Kitts and Nevis</td><td>145</td></tr><tr><td>Uruguay</td><td>United States</td><td>43</td></tr><tr><td>United States</td><td>Haiti</td><td>225</td></tr><tr><td>Bonaire, Sint Eustatius, and Saba</td><td>United States</td><td>58</td></tr><tr><td>Greece</td><td>United States</td><td>30</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "United States",
         "Romania",
         "15"
        ],
        [
         "United States",
         "Croatia",
         "1"
        ],
        [
         "United States",
         "Ireland",
         "344"
        ],
        [
         "Egypt",
         "United States",
         "15"
        ],
        [
         "United States",
         "India",
         "62"
        ],
        [
         "United States",
         "Singapore",
         "1"
        ],
        [
         "United States",
         "Grenada",
         "62"
        ],
        [
         "Costa Rica",
         "United States",
         "588"
        ],
        [
         "Senegal",
         "United States",
         "40"
        ],
        [
         "Moldova",
         "United States",
         "1"
        ],
        [
         "United States",
         "Sint Maarten",
         "325"
        ],
        [
         "United States",
         "Marshall Islands",
         "39"
        ],
        [
         "Guyana",
         "United States",
         "64"
        ],
        [
         "Malta",
         "United States",
         "1"
        ],
        [
         "Anguilla",
         "United States",
         "41"
        ],
        [
         "Bolivia",
         "United States",
         "30"
        ],
        [
         "United States",
         "Paraguay",
         "6"
        ],
        [
         "Algeria",
         "United States",
         "4"
        ],
        [
         "Turks and Caicos Islands",
         "United States",
         "230"
        ],
        [
         "United States",
         "Gibraltar",
         "1"
        ],
        [
         "Saint Vincent and the Grenadines",
         "United States",
         "1"
        ],
        [
         "Italy",
         "United States",
         "382"
        ],
        [
         "United States",
         "Federated States of Micronesia",
         "69"
        ],
        [
         "United States",
         "Russia",
         "161"
        ],
        [
         "Pakistan",
         "United States",
         "12"
        ],
        [
         "United States",
         "Netherlands",
         "660"
        ],
        [
         "Iceland",
         "United States",
         "181"
        ],
        [
         "Marshall Islands",
         "United States",
         "42"
        ],
        [
         "Luxembourg",
         "United States",
         "155"
        ],
        [
         "Honduras",
         "United States",
         "362"
        ],
        [
         "The Bahamas",
         "United States",
         "955"
        ],
        [
         "United States",
         "Senegal",
         "42"
        ],
        [
         "El Salvador",
         "United States",
         "561"
        ],
        [
         "Samoa",
         "United States",
         "25"
        ],
        [
         "United States",
         "Angola",
         "13"
        ],
        [
         "Switzerland",
         "United States",
         "294"
        ],
        [
         "United States",
         "Anguilla",
         "38"
        ],
        [
         "Sint Maarten",
         "United States",
         "325"
        ],
        [
         "Hong Kong",
         "United States",
         "332"
        ],
        [
         "Trinidad and Tobago",
         "United States",
         "211"
        ],
        [
         "Latvia",
         "United States",
         "19"
        ],
        [
         "United States",
         "Ecuador",
         "300"
        ],
        [
         "Suriname",
         "United States",
         "1"
        ],
        [
         "Mexico",
         "United States",
         "7140"
        ],
        [
         "United States",
         "Cyprus",
         "1"
        ],
        [
         "Ecuador",
         "United States",
         "268"
        ],
        [
         "United States",
         "Portugal",
         "134"
        ],
        [
         "United States",
         "Costa Rica",
         "608"
        ],
        [
         "United States",
         "Guatemala",
         "318"
        ],
        [
         "United States",
         "Suriname",
         "34"
        ],
        [
         "Colombia",
         "United States",
         "873"
        ],
        [
         "United States",
         "Cape Verde",
         "14"
        ],
        [
         "United States",
         "Jamaica",
         "712"
        ],
        [
         "Norway",
         "United States",
         "121"
        ],
        [
         "United States",
         "Malaysia",
         "3"
        ],
        [
         "United States",
         "Morocco",
         "19"
        ],
        [
         "Thailand",
         "United States",
         "3"
        ],
        [
         "United States",
         "Samoa",
         "25"
        ],
        [
         "Venezuela",
         "United States",
         "290"
        ],
        [
         "United States",
         "Palau",
         "31"
        ],
        [
         "United States",
         "Venezuela",
         "246"
        ],
        [
         "Panama",
         "United States",
         "510"
        ],
        [
         "Antigua and Barbuda",
         "United States",
         "126"
        ],
        [
         "United States",
         "Chile",
         "185"
        ],
        [
         "Morocco",
         "United States",
         "15"
        ],
        [
         "United States",
         "Finland",
         "28"
        ],
        [
         "Azerbaijan",
         "United States",
         "21"
        ],
        [
         "United States",
         "Greece",
         "23"
        ],
        [
         "United States",
         "The Bahamas",
         "986"
        ],
        [
         "New Zealand",
         "United States",
         "111"
        ],
        [
         "Liberia",
         "United States",
         "2"
        ],
        [
         "United States",
         "Hong Kong",
         "414"
        ],
        [
         "Hungary",
         "United States",
         "2"
        ],
        [
         "United States",
         "China",
         "920"
        ],
        [
         "United States",
         "Vietnam",
         "2"
        ],
        [
         "Burkina Faso",
         "United States",
         "1"
        ],
        [
         "Sweden",
         "United States",
         "118"
        ],
        [
         "United States",
         "Kuwait",
         "28"
        ],
        [
         "United States",
         "Dominican Republic",
         "1420"
        ],
        [
         "United States",
         "Egypt",
         "12"
        ],
        [
         "Israel",
         "United States",
         "134"
        ],
        [
         "United States",
         "United States",
         "370002"
        ],
        [
         "Ethiopia",
         "United States",
         "13"
        ],
        [
         "United States",
         "Luxembourg",
         "134"
        ],
        [
         "United States",
         "Poland",
         "33"
        ],
        [
         "Martinique",
         "United States",
         "44"
        ],
        [
         "United States",
         "Saint Barthelemy",
         "41"
        ],
        [
         "Saint Barthelemy",
         "United States",
         "39"
        ],
        [
         "Barbados",
         "United States",
         "154"
        ],
        [
         "United States",
         "Turkey",
         "129"
        ],
        [
         "Djibouti",
         "United States",
         "1"
        ],
        [
         "United States",
         "Azerbaijan",
         "21"
        ],
        [
         "United States",
         "Estonia",
         "1"
        ],
        [
         "Germany",
         "United States",
         "1468"
        ],
        [
         "United States",
         "South Korea",
         "827"
        ],
        [
         "United States",
         "El Salvador",
         "508"
        ],
        [
         "Ireland",
         "United States",
         "335"
        ],
        [
         "United States",
         "Hungary",
         "3"
        ],
        [
         "Zambia",
         "United States",
         "1"
        ],
        [
         "Malaysia",
         "United States",
         "2"
        ],
        [
         "United States",
         "Ethiopia",
         "12"
        ],
        [
         "United States",
         "Panama",
         "465"
        ],
        [
         "United States",
         "Aruba",
         "342"
        ],
        [
         "United States",
         "Thailand",
         "4"
        ],
        [
         "United States",
         "Turks and Caicos Islands",
         "236"
        ],
        [
         "Croatia",
         "United States",
         "2"
        ],
        [
         "United States",
         "Pakistan",
         "12"
        ],
        [
         "Cyprus",
         "United States",
         "1"
        ],
        [
         "United States",
         "Honduras",
         "407"
        ],
        [
         "Fiji",
         "United States",
         "24"
        ],
        [
         "Qatar",
         "United States",
         "108"
        ],
        [
         "Saint Kitts and Nevis",
         "United States",
         "139"
        ],
        [
         "Kuwait",
         "United States",
         "32"
        ],
        [
         "Taiwan",
         "United States",
         "266"
        ],
        [
         "Haiti",
         "United States",
         "226"
        ],
        [
         "Canada",
         "United States",
         "8399"
        ],
        [
         "Federated States of Micronesia",
         "United States",
         "69"
        ],
        [
         "United States",
         "Liberia",
         "2"
        ],
        [
         "Jamaica",
         "United States",
         "666"
        ],
        [
         "United States",
         "Malta",
         "2"
        ],
        [
         "Dominican Republic",
         "United States",
         "1353"
        ],
        [
         "Japan",
         "United States",
         "1548"
        ],
        [
         "United States",
         "Lithuania",
         "1"
        ],
        [
         "Finland",
         "United States",
         "26"
        ],
        [
         "United States",
         "Guadeloupe",
         "59"
        ],
        [
         "United States",
         "Ukraine",
         "13"
        ],
        [
         "United States",
         "France",
         "952"
        ],
        [
         "United States",
         "Norway",
         "115"
        ],
        [
         "Aruba",
         "United States",
         "346"
        ],
        [
         "French Guiana",
         "United States",
         "5"
        ],
        [
         "United States",
         "Kiribati",
         "35"
        ],
        [
         "India",
         "United States",
         "61"
        ],
        [
         "British Virgin Islands",
         "United States",
         "107"
        ],
        [
         "Brazil",
         "United States",
         "853"
        ],
        [
         "United States",
         "Germany",
         "1336"
        ],
        [
         "United States",
         "New Zealand",
         "74"
        ],
        [
         "French Polynesia",
         "United States",
         "43"
        ],
        [
         "United Arab Emirates",
         "United States",
         "320"
        ],
        [
         "Singapore",
         "United States",
         "3"
        ],
        [
         "United States",
         "Mexico",
         "7187"
        ],
        [
         "United States",
         "Sweden",
         "119"
        ],
        [
         "Netherlands",
         "United States",
         "776"
        ],
        [
         "United States",
         "Martinique",
         "43"
        ],
        [
         "United States",
         "United Arab Emirates",
         "313"
        ],
        [
         "United States",
         "Bulgaria",
         "1"
        ],
        [
         "Denmark",
         "United States",
         "153"
        ],
        [
         "China",
         "United States",
         "772"
        ],
        [
         "United States",
         "Nicaragua",
         "201"
        ],
        [
         "United States",
         "Philippines",
         "126"
        ],
        [
         "United States",
         "Georgia",
         "1"
        ],
        [
         "United States",
         "Belgium",
         "228"
        ],
        [
         "Cayman Islands",
         "United States",
         "314"
        ],
        [
         "Argentina",
         "United States",
         "180"
        ],
        [
         "Peru",
         "United States",
         "279"
        ],
        [
         "South Africa",
         "United States",
         "36"
        ],
        [
         "United States",
         "Iceland",
         "202"
        ],
        [
         "United States",
         "Argentina",
         "141"
        ],
        [
         "Spain",
         "United States",
         "420"
        ],
        [
         "Bermuda",
         "United States",
         "183"
        ],
        [
         "United States",
         "Nigeria",
         "50"
        ],
        [
         "United States",
         "Austria",
         "63"
        ],
        [
         "United States",
         "Bonaire, Sint Eustatius, and Saba",
         "59"
        ],
        [
         "Kiribati",
         "United States",
         "26"
        ],
        [
         "Saudi Arabia",
         "United States",
         "83"
        ],
        [
         "Czech Republic",
         "United States",
         "13"
        ],
        [
         "United States",
         "Israel",
         "127"
        ],
        [
         "Belgium",
         "United States",
         "259"
        ],
        [
         "United States",
         "Saint Lucia",
         "136"
        ],
        [
         "United States",
         "Bahrain",
         "1"
        ],
        [
         "United States",
         "British Virgin Islands",
         "80"
        ],
        [
         "Curacao",
         "United States",
         "90"
        ],
        [
         "Georgia",
         "United States",
         "2"
        ],
        [
         "United States",
         "Denmark",
         "152"
        ],
        [
         "United States",
         "Guyana",
         "63"
        ],
        [
         "Philippines",
         "United States",
         "134"
        ],
        [
         "Grenada",
         "United States",
         "53"
        ],
        [
         "Cape Verde",
         "United States",
         "20"
        ],
        [
         "Cote d'Ivoire",
         "United States",
         "1"
        ],
        [
         "Ukraine",
         "United States",
         "14"
        ],
        [
         "United States",
         "Papua New Guinea",
         "1"
        ],
        [
         "Russia",
         "United States",
         "176"
        ],
        [
         "United States",
         "Saudi Arabia",
         "70"
        ],
        [
         "Guatemala",
         "United States",
         "397"
        ],
        [
         "Saint Lucia",
         "United States",
         "123"
        ],
        [
         "Paraguay",
         "United States",
         "60"
        ],
        [
         "United States",
         "Curacao",
         "83"
        ],
        [
         "Kosovo",
         "United States",
         "1"
        ],
        [
         "United States",
         "Taiwan",
         "235"
        ],
        [
         "Tunisia",
         "United States",
         "3"
        ],
        [
         "United States",
         "South Africa",
         "40"
        ],
        [
         "Niger",
         "United States",
         "2"
        ],
        [
         "Turkey",
         "United States",
         "138"
        ],
        [
         "United Kingdom",
         "United States",
         "2025"
        ],
        [
         "Romania",
         "United States",
         "14"
        ],
        [
         "United States",
         "Greenland",
         "4"
        ],
        [
         "Papua New Guinea",
         "United States",
         "3"
        ],
        [
         "United States",
         "Spain",
         "442"
        ],
        [
         "Iraq",
         "United States",
         "1"
        ],
        [
         "United States",
         "Italy",
         "438"
        ],
        [
         "Cuba",
         "United States",
         "466"
        ],
        [
         "United States",
         "Switzerland",
         "305"
        ],
        [
         "Dominica",
         "United States",
         "20"
        ],
        [
         "United States",
         "Japan",
         "1496"
        ],
        [
         "Portugal",
         "United States",
         "127"
        ],
        [
         "United States",
         "Brazil",
         "619"
        ],
        [
         "Bahrain",
         "United States",
         "19"
        ],
        [
         "United States",
         "Peru",
         "337"
        ],
        [
         "Indonesia",
         "United States",
         "1"
        ],
        [
         "United States",
         "Belize",
         "193"
        ],
        [
         "United States",
         "United Kingdom",
         "1970"
        ],
        [
         "Belize",
         "United States",
         "188"
        ],
        [
         "United States",
         "Ghana",
         "20"
        ],
        [
         "United States",
         "Indonesia",
         "2"
        ],
        [
         "United States",
         "Fiji",
         "25"
        ],
        [
         "United States",
         "Canada",
         "8483"
        ],
        [
         "United States",
         "Antigua and Barbuda",
         "117"
        ],
        [
         "United States",
         "French Polynesia",
         "40"
        ],
        [
         "Nicaragua",
         "United States",
         "179"
        ],
        [
         "United States",
         "Latvia",
         "15"
        ],
        [
         "United States",
         "Dominica",
         "27"
        ],
        [
         "United States",
         "Czech Republic",
         "12"
        ],
        [
         "United States",
         "Australia",
         "258"
        ],
        [
         "United States",
         "Cook Islands",
         "13"
        ],
        [
         "Austria",
         "United States",
         "62"
        ],
        [
         "Jordan",
         "United States",
         "44"
        ],
        [
         "Palau",
         "United States",
         "30"
        ],
        [
         "South Korea",
         "United States",
         "1048"
        ],
        [
         "Angola",
         "United States",
         "15"
        ],
        [
         "Ghana",
         "United States",
         "18"
        ],
        [
         "New Caledonia",
         "United States",
         "1"
        ],
        [
         "Guadeloupe",
         "United States",
         "56"
        ],
        [
         "France",
         "United States",
         "935"
        ],
        [
         "Poland",
         "United States",
         "32"
        ],
        [
         "Nigeria",
         "United States",
         "59"
        ],
        [
         "United States",
         "Uruguay",
         "13"
        ],
        [
         "Greenland",
         "United States",
         "2"
        ],
        [
         "United States",
         "Bermuda",
         "193"
        ],
        [
         "Chile",
         "United States",
         "174"
        ],
        [
         "United States",
         "Cuba",
         "478"
        ],
        [
         "United States",
         "Montenegro",
         "1"
        ],
        [
         "United States",
         "Colombia",
         "867"
        ],
        [
         "United States",
         "Barbados",
         "130"
        ],
        [
         "United States",
         "Qatar",
         "109"
        ],
        [
         "Australia",
         "United States",
         "329"
        ],
        [
         "United States",
         "Cayman Islands",
         "310"
        ],
        [
         "United States",
         "Jordan",
         "44"
        ],
        [
         "United States",
         "Namibia",
         "1"
        ],
        [
         "United States",
         "Trinidad and Tobago",
         "217"
        ],
        [
         "United States",
         "Bolivia",
         "13"
        ],
        [
         "Cook Islands",
         "United States",
         "13"
        ],
        [
         "Bulgaria",
         "United States",
         "3"
        ],
        [
         "United States",
         "Saint Kitts and Nevis",
         "145"
        ],
        [
         "Uruguay",
         "United States",
         "43"
        ],
        [
         "United States",
         "Haiti",
         "225"
        ],
        [
         "Bonaire, Sint Eustatius, and Saba",
         "United States",
         "58"
        ],
        [
         "Greece",
         "United States",
         "30"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEST_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_COUNTRY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2809f829-88f2-4812-8238-ffec73bb57bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StructType,StringType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83daa3d8-f26b-4adf-99fc-fb27f9cd8130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54697912-24e1-4339-9ea2-0434de2c3bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema=StructType(\n",
    "    [\n",
    "        StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "        StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "        StructField(\"count\",IntegerType(),True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a489ef5-949f-4f98-8e82-706038662c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-4490155068148989>:6\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n",
       "\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferschema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n",
       "\u001b[1;32m      3\u001b[0m      \u001b[38;5;241m.\u001b[39mschema(my_schema)\\\n",
       "\u001b[1;32m      4\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n",
       "\u001b[1;32m      5\u001b[0m         \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/FileStore/tables/2015_summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[0;32m----> 6\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n",
       "\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n",
       "\u001b[1;32m    915\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_A_BOOLEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m    916\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n",
       "\u001b[1;32m    917\u001b[0m     )\n",
       "\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n",
       "\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2309.showString.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (ip-10-172-164-232.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2015_summary.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\t... 32 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"count\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n",
       "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4382)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3168)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3168)\n",
       "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3389)\n",
       "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:315)\n",
       "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:354)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2015_summary.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\t... 32 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"count\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-4490155068148989>:6\u001b[0m\n\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferschema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m      \u001b[38;5;241m.\u001b[39mschema(my_schema)\\\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/FileStore/tables/2015_summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    915\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_A_BOOLEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    916\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    917\u001b[0m     )\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2309.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (ip-10-172-164-232.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2015_summary.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 32 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\nCaused by: java.lang.NumberFormatException: For input string: \"count\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4382)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:315)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:354)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2015_summary.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 32 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\nCaused by: java.lang.NumberFormatException: For input string: \"count\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (ip-10-172-164-232.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2015_summary.csv.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"false\")\\\n",
    "# Setting read options\n",
    "    .option(\"inferschema\",\"false\")\\\n",
    "# Defining schema manually\n",
    "     .schema(my_schema)\\\n",
    "# Setting read options\n",
    "        .option(\"mode\",\"FAILFAST\")\\\n",
    "# Loading the data from specified path\n",
    "        .load(\"/FileStore/tables/2015_summary.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c96754-9099-4692-940d-ef61cb81575d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"false\")\\\n",
    "# Setting read options\n",
    "    .option(\"inferschema\",\"false\")\\\n",
    "# Defining schema manually\n",
    "     .schema(my_schema)\\\n",
    "# Setting read options\n",
    "        .option(\"mode\",\"PERMISSIVE\")\\\n",
    "# Loading the data from specified path\n",
    "        .load(\"/FileStore/tables/2015_summary.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa82dd7-d174-4c37-a355-7ef98058e941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"false\")\\\n",
    "# Setting read options\n",
    "    .option(\"inferschema\",\"false\")\\\n",
    "# Setting read options\n",
    "     .option(\"skipRows\",1)\\\n",
    "# Defining schema manually\n",
    "     .schema(my_schema)\\\n",
    "# Setting read options\n",
    "        .option(\"mode\",\"PERMISSIVE\")\\\n",
    "# Loading the data from specified path\n",
    "        .load(\"/FileStore/tables/2015_summary.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52cb8e87-2c44-4767-911a-34df9a690b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"false\")\\\n",
    "# Setting read options\n",
    "    .option(\"inferschema\",\"false\")\\\n",
    "# Setting read options\n",
    "     .option(\"skipRows\",1)\\\n",
    "# Defining schema manually\n",
    "     .schema(my_schema)\\\n",
    "# Setting read options\n",
    "        .option(\"mode\",\"FAILFAST\")\\\n",
    "# Loading the data from specified path\n",
    "        .load(\"/FileStore/tables/2015_summary.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59a64e6e-e2a8-408a-ad6a-9665e8408d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DDL stands for Data Definition Language, and it comes from the good ol‚Äô SQL world where you use commands like CREATE TABLE to define schemas and data types.\n",
    "\n",
    "In Spark, a DDL-style schema is a string-based representation of a schema that looks like the column definitions you'd find in a SQL CREATE TABLE statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4608e207-8d0d-4a75-8b7e-c9afb0f68758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|Count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddl_schema = \"DEST_COUNTRY_NAME STRING,  ORIGIN_COUNTRY_NAME STRING,Count INT\"\n",
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"false\")\\\n",
    "# Setting read options\n",
    "    .option(\"inferschema\",\"false\")\\\n",
    "# Setting read options\n",
    "     .option(\"skipRows\",1)\\\n",
    "# Defining schema manually\n",
    "     .schema(ddl_schema)\\\n",
    "# Setting read options\n",
    "        .option(\"mode\",\"FAILFAST\")\\\n",
    "# Loading the data from specified path\n",
    "        .load(\"/FileStore/tables/2015_summary.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a3a59ad-43a8-4cfd-b37d-232195a4a15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#handling Corrupted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248e46ea-b47c-49c1-8a3f-253d96c4f360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-334292990152322>:5\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n",
       "\u001b[1;32m      2\u001b[0m              \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferschema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n",
       "\u001b[1;32m      3\u001b[0m             \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n",
       "\u001b[1;32m      4\u001b[0m             \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/FileStore/tables/Employee.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[0;32m----> 5\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n",
       "\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n",
       "\u001b[1;32m    915\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_A_BOOLEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m    916\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n",
       "\u001b[1;32m    917\u001b[0m     )\n",
       "\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n",
       "\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o661.showString.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (ip-10-172-174-145.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/Employee.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\t... 28 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3\n",
       "Caused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1703)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:346)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n",
       "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4382)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3168)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3168)\n",
       "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3389)\n",
       "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:315)\n",
       "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:354)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/Employee.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\t... 28 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3\n",
       "Caused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1703)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:346)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-334292990152322>:5\u001b[0m\n\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      2\u001b[0m              \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferschema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/FileStore/tables/Employee.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    915\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_A_BOOLEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    916\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    917\u001b[0m     )\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o661.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (ip-10-172-174-145.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/Employee.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 28 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1703)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:346)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4382)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:315)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:354)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/Employee.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 28 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1703)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:346)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (ip-10-172-174-145.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/Employee.csv.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "#FailFast method Fails as soon as it encounnters bad data\n",
    "df=spark.read.format(\"csv\")\\\n",
    "# Setting read options\n",
    "             .option(\"inferschema\",\"true\")\\\n",
    "# Setting read options\n",
    "            .option(\"mode\",\"FAILFAST\")\\\n",
    "# Loading the data from specified path\n",
    "            .load(\"/FileStore/tables/Employee.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3139176-657a-48bb-9210-3a04dbfc35d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n",
      "| id|    name|age|salary|     address| nominee|\n",
      "+---+--------+---+------+------------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|\n",
      "|  5|  Vikash| 31|300000|        null|nominee5|\n",
      "+---+--------+---+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "#Permissive method doesnt fail but it gives bad record\n",
    "df=spark.read.format(\"csv\")\\\n",
    "# Setting read options\n",
    "             .option(\"inferschema\",\"true\")\\\n",
    "# Setting read options\n",
    "              .option(\"header\",\"true\")\\\n",
    "# Setting read options\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "# Loading the data from specified path\n",
    "            .load(\"/FileStore/tables/Employee.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbcae4b-2fe4-49a6-a5d5-e54a5db39e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+\n",
      "| id|  name|age|salary|     address| nominee|\n",
      "+---+------+---+------+------------+--------+\n",
      "|  1|Manish| 26| 75000|       bihar|nominee1|\n",
      "|  2|Nikita| 23|100000|uttarpradesh|nominee2|\n",
      "|  5|Vikash| 31|300000|        null|nominee5|\n",
      "+---+------+---+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "#dropmalformed drops the corrupt part\n",
    "df=spark.read.format(\"csv\")\\\n",
    "# Setting read options\n",
    "             .option(\"inferschema\",\"true\")\\\n",
    "# Setting read options\n",
    "              .option(\"header\",\"true\")\\\n",
    "# Setting read options\n",
    "            .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "# Loading the data from specified path\n",
    "            .load(\"/FileStore/tables/Employee.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa3aa0f-36ce-4c9f-a176-f0f1a419f3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59bbd2e7-0037-4360-a9e5-06072581cfb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema=StructType(\n",
    "              [\n",
    "                  StructField(\"ID\",IntegerType(),True),\n",
    "                  StructField(\"name\",StringType(),True),\n",
    "                  StructField(\"age\",IntegerType(),True),\n",
    "                  StructField(\"Salary\",IntegerType(),True),\n",
    "                  StructField(\"address\",StringType(),True),\n",
    "                  StructField(\"nominee\",StringType(),True),\n",
    "                  StructField(\"_corrupt_record\",StringType(),True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "              ]\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "954d9777-6f21-4712-9d2d-f6c58deaffed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------+\n",
      "| ID|    name|age|Salary|     address| nominee|Corrupt_Record|\n",
      "+---+--------+---+------+------------+--------+--------------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|          null|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|          null|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|      nominee3|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|      nominee4|\n",
      "|  5|  Vikash| 31|300000|        null|nominee5|          null|\n",
      "+---+--------+---+------+------------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\")\\\n",
    "# Setting read options\n",
    "             .option(\"inferschema\",\"false\")\\\n",
    "# Setting read options\n",
    "              .option(\"header\",\"true\")\\\n",
    "# Defining schema manually\n",
    "               .schema(schema)\\\n",
    "# Setting read options\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "# Loading the data from specified path\n",
    "            .load(\"/FileStore/tables/Employee.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4b25e3-6a02-48ce-98e5-8ca2fba21ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------+\n",
      "|ID |name    |age|Salary|address     |nominee |Corrupt_Record|\n",
      "+---+--------+---+------+------------+--------+--------------+\n",
      "|1  |Manish  |26 |75000 |bihar       |nominee1|null          |\n",
      "|2  |Nikita  |23 |100000|uttarpradesh|nominee2|null          |\n",
      "|3  |Pritam  |22 |150000|Bangalore   |India   |nominee3      |\n",
      "|4  |Prantosh|17 |200000|Kolkata     |India   |nominee4      |\n",
      "|5  |Vikash  |31 |300000|null        |nominee5|null          |\n",
      "+---+--------+---+------+------------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\")\\\n",
    "# Setting read options\n",
    "             .option(\"inferschema\",\"true\")\\\n",
    "# Setting read options\n",
    "              .option(\"header\",\"true\")\\\n",
    "# Defining schema manually\n",
    "               .schema(schema)\\\n",
    "# Setting read options\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "# Loading the data from specified path\n",
    "            .load(\"/FileStore/tables/Employee.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show(truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d4ab88-0556-40ac-8a44-ea3b85d4255b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "|ID |name    |age|Salary|address     |nominee |_corrupt_record                            |\n",
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "|1  |Manish  |26 |75000 |bihar       |nominee1|null                                       |\n",
      "|2  |Nikita  |23 |100000|uttarpradesh|nominee2|null                                       |\n",
      "|3  |Pritam  |22 |150000|Bangalore   |India   |3,Pritam,22,150000,Bangalore,India,nominee3|\n",
      "|4  |Prantosh|17 |200000|Kolkata     |India   |4,Prantosh,17,200000,Kolkata,India,nominee4|\n",
      "|5  |Vikash  |31 |300000|null        |nominee5|null                                       |\n",
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\")\\\n",
    "# Setting read options\n",
    "             .option(\"inferschema\",\"true\")\\\n",
    "# Setting read options\n",
    "              .option(\"header\",\"true\")\\\n",
    "# Defining schema manually\n",
    "               .schema(schema)\\\n",
    "# Setting read options\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "# Loading the data from specified path\n",
    "            .load(\"/FileStore/tables/Employee.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show(truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ba7739-3b2b-48d7-a395-0ee07b24468d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "|ID |name    |age|Salary|address     |nominee |_corrupt_record                            |\n",
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "|1  |Manish  |26 |75000 |bihar       |nominee1|null                                       |\n",
      "|2  |Nikita  |23 |100000|uttarpradesh|nominee2|null                                       |\n",
      "|3  |Pritam  |22 |150000|Bangalore   |India   |3,Pritam,22,150000,Bangalore,India,nominee3|\n",
      "|4  |Prantosh|17 |200000|Kolkata     |India   |4,Prantosh,17,200000,Kolkata,India,nominee4|\n",
      "|5  |Vikash  |31 |300000|null        |nominee5|null                                       |\n",
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\")\\\n",
    "# Setting read options\n",
    "             .option(\"inferschema\",\"false\")\\\n",
    "# Setting read options\n",
    "              .option(\"header\",\"true\")\\\n",
    "# Defining schema manually\n",
    "               .schema(schema)\\\n",
    "# Setting read options\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "# Loading the data from specified path\n",
    "            .load(\"/FileStore/tables/Employee.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show(truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba202c80-2e44-4e11-859c-e491c66154f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: [FileInfo(path='dbfs:/FileStore/', name='FileStore/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/databricks-datasets/', name='databricks-datasets/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/databricks-results/', name='databricks-results/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/mnt/', name='mnt/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/user/', name='user/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "# Listing files in DBFS directory\n",
    "dbutils.fs.ls('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2136c57-d3fc-46a8-94ea-8ef40f883cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+---------------+\n",
      "|ID |name  |age|Salary|address     |nominee |_corrupt_record|\n",
      "+---+------+---+------+------------+--------+---------------+\n",
      "|1  |Manish|26 |75000 |bihar       |nominee1|null           |\n",
      "|2  |Nikita|23 |100000|uttarpradesh|nominee2|null           |\n",
      "|5  |Vikash|31 |300000|null        |nominee5|null           |\n",
      "+---+------+---+------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data using Spark DataFrame Reader\n",
    "df=spark.read.format(\"csv\")\\\n",
    "# Setting read options\n",
    "             .option(\"inferschema\",\"false\")\\\n",
    "# Setting read options\n",
    "              .option(\"header\",\"true\")\\\n",
    "# Defining schema manually\n",
    "               .schema(schema)\\\n",
    "# Setting read options\n",
    "            .option(\"badRecordsPath\",\"/FileStore/tables/bad_recods\")\\\n",
    "# Loading the data from specified path\n",
    "            .load(\"/FileStore/tables/Employee.csv\")\n",
    "# Displaying the DataFrame content\n",
    "df.show(truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee995b9-ce02-47c8-99ba-09982473b9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: [FileInfo(path='dbfs:/FileStore/tables/bad_recods/20250420T094451/', name='20250420T094451/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "# Listing files in DBFS directory\n",
    "dbutils.fs.ls('/FileStore/tables/bad_recods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f762eadc-1432-4bb7-8682-b59b4a9ba7e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250420T092938/\n"
     ]
    }
   ],
   "source": [
    "# Listing files in DBFS directory\n",
    "files = dbutils.fs.ls(\"/FileStore/tables/bad_records\")\n",
    "for f in files:\n",
    "    print(f.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e314bd8-76c0-41a7-b574-d32b78f2c1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: [FileInfo(path='dbfs:/FileStore/tables/bad_records/20250420T092938/bad_records/', name='bad_records/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "# Listing files in DBFS directory\n",
    "dbutils.fs.ls(\"/FileStore/tables/bad_records/20250420T092938\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396f0670-ea6b-437f-a2b5-739f1f47975a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/bad_recods/20250420T094451/bad_records/part-00000-4c1e22bb-622b-4b94-b5b8-d0f018ea03ed</td><td>part-00000-4c1e22bb-622b-4b94-b5b8-d0f018ea03ed</td><td>484</td><td>1745142292000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/bad_recods/20250420T094451/bad_records/part-00000-4c1e22bb-622b-4b94-b5b8-d0f018ea03ed",
         "part-00000-4c1e22bb-622b-4b94-b5b8-d0f018ea03ed",
         484,
         1745142292000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls      /FileStore/tables/bad_recods/20250420T094451/bad_records/                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93f63e0-cda3-4b67-a769-31a61d9ed224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n",
      "|path                               |reason                                                                                                                          |record                                     |\n",
      "+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n",
      "|dbfs:/FileStore/tables/Employee.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3|3,Pritam,22,150000,Bangalore,India,nominee3|\n",
      "|dbfs:/FileStore/tables/Employee.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 4,Prantosh,17,200000,Kolkata,India,nominee4|4,Prantosh,17,200000,Kolkata,India,nominee4|\n",
      "+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace with the actual subfolder inside bad_records\n",
    "# Reading data using Spark DataFrame Reader\n",
    "bad_data_df = spark.read.format(\"json\").load(\"/FileStore/tables/bad_recods/20250420T094451/bad_records\")\n",
    "# Displaying the DataFrame content\n",
    "bad_data_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabbefd9-f839-4e55-8462-4f9a3b67e802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>reason</th><th>record</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/Employee.csv</td><td>org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3</td><td>3,Pritam,22,150000,Bangalore,India,nominee3</td></tr><tr><td>dbfs:/FileStore/tables/Employee.csv</td><td>org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 4,Prantosh,17,200000,Kolkata,India,nominee4</td><td>4,Prantosh,17,200000,Kolkata,India,nominee4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/Employee.csv",
         "org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3",
         "3,Pritam,22,150000,Bangalore,India,nominee3"
        ],
        [
         "dbfs:/FileStore/tables/Employee.csv",
         "org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 4,Prantosh,17,200000,Kolkata,India,nominee4",
         "4,Prantosh,17,200000,Kolkata,India,nominee4"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "reason",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "record",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(bad_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e31ff2cc-b23b-44bf-9325-779e47027819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\\\n",
    "\\"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2689680928244102,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_basic_day_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
